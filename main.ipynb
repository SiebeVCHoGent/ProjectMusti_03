{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import os\n",
    "import tarfile\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set numpy's random-state to 42 to make this notebook's output stable across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reload data** decides if a new dataframe has to be created. \\\n",
    "Default is False, but will be set to True if no dataframe exists yet.\\\n",
    "Can be set to True to force the program to overwrite an existing dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function 'target_value'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_value(val):\n",
    "    if val == 'aanwezig':\n",
    "        return 2\n",
    "    if val == 'buiten':\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if directory 'model' exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./model/'):\n",
    "    os.mkdir('./model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a dataframe exists, and create a new one if this is not the case.\n",
    "Else load the existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataFrame\n",
      "DataFrame Loaded\n",
      "      target   p1   p2   p3   p4   p5   p6   p7   p8   p9  ...  p56311  \\\n",
      "0          2  107  116  116  110  110  106  110  111  109  ...      99   \n",
      "1          2  107  116  116  110  111  108  111  113  107  ...     100   \n",
      "2          2  107  116  116  110  111  107  109  110  107  ...     -51   \n",
      "3          2  109  121  122  113  113  112  115  111  109  ...     -47   \n",
      "4          2  107  119  120  111  113  109  110  112  107  ...     -50   \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
      "2635       0  107  116  116  110  110  109  113  113  107  ...      42   \n",
      "2636       0   92   92   95   93   91   91   93   98   92  ...      72   \n",
      "2637       0   95  105  100   96   94   94   95  102   96  ...      76   \n",
      "2638       0   97  108  105  100  100   98  100  108   98  ...      80   \n",
      "2639       0   70   75   75   76   76   78   80   82   75  ...      81   \n",
      "\n",
      "      p56312  p56313  p56314  p56315  p56316  p56317  p56318  p56319  p56320  \n",
      "0         53      37      35      36      38      38      46      51      50  \n",
      "1         51      39      36      35      36      43      47      50      51  \n",
      "2        127      92      87      90     101     109     123    -115    -121  \n",
      "3       -125      97      95      97     103     113    -128    -109    -113  \n",
      "4        126      94      90      92     100     109     125    -119    -118  \n",
      "...      ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
      "2635      47      57      70      79      84      94    -126    -127     121  \n",
      "2636      78      87      98     107     115     126     -99    -103    -112  \n",
      "2637      82      93     107     117     124    -123     -90     -95    -107  \n",
      "2638      88      95     109     119     126    -118     -84     -86     -96  \n",
      "2639      87     100     111     119     126    -116     -83     -87     -98  \n",
      "\n",
      "[2640 rows x 56321 columns]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('./model/dataframe.sav'):\n",
    "    RELOAD_DATA = True # When there is not yet a dataframe, create one\n",
    "\n",
    "if RELOAD_DATA: # Check whether there needs to be created a new dataframe\n",
    "    #Extract when not already extracted\n",
    "    if not os.path.isdir('./data/classificatie'):\n",
    "        if not os.path.isfile('./data/classificatie.tar'):\n",
    "            raise Exception('Classificatie.tar not fount')\n",
    "\n",
    "        print('Extracting tar...')\n",
    "        tar = tarfile.open('./data/classificatie.tar')\n",
    "        tar.extractall('./data/')\n",
    "        tar.close()\n",
    "        print('Extracting tar Done!')\n",
    "\n",
    "    if not os.path.isdir('./data/classificatie'):\n",
    "        raise Exception('Extracted files not found')\n",
    "\n",
    "\n",
    "    # Get grayscale values from pictures\n",
    "    print('Creating dataframe')\n",
    "    samples = []\n",
    "    sample_counter = 0\n",
    "    musti = pd.DataFrame()\n",
    "\n",
    "    for folder in os.listdir('./data/classificatie/'):\n",
    "        for file in os.listdir(f'./data/classificatie/{folder}'):\n",
    "            if target_value(folder) == 1 or randint(1, 5) ==1:\n",
    "                img = cv2.imread(f'./data/classificatie/{folder}/{file}', 0)\n",
    "                img = cv2.resize(img, (320, 176))\n",
    "\n",
    "                # add them to a dataframe\n",
    "                imgd = dict()\n",
    "                imgd['target'] = target_value(folder)\n",
    "                c = 0\n",
    "                for i in img.flatten():\n",
    "                    c += 1\n",
    "                    imgd[f'p{c}'] = i\n",
    "                samples.append(imgd)\n",
    "                sample_counter+=1\n",
    "                #print(file)\n",
    "\n",
    "                if sample_counter % 200==0:\n",
    "                    temp_df = pd.DataFrame.from_dict(samples)\n",
    "                    musti = musti.append(temp_df, ignore_index=True)\n",
    "                    samples = []\n",
    "    temp_df = pd.DataFrame.from_dict(samples)\n",
    "    musti = musti.append(temp_df, ignore_index=True)\n",
    "    samples = []\n",
    "\n",
    "    print('Saving DataFrame')\n",
    "    \n",
    "    pickle.dump(musti, open('./model/dataframe.sav', 'wb'))\n",
    "else:\n",
    "    print('Loading DataFrame')\n",
    "    musti = pickle.load(open('./model/dataframe.sav', 'rb'))\n",
    "\n",
    "print('DataFrame Loaded')\n",
    "print(musti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7219442458899215 0\n",
      "0.0 1\n",
      "0.5434272300469484 2\n",
      "389 389 389\n"
     ]
    }
   ],
   "source": [
    "smallest_dataset_len = min(len(musti[musti.target == 2]), len(musti[musti.target == 1]), len(musti[musti.target == 0]))\n",
    "for i in range(3):\n",
    "    frac = 1 - smallest_dataset_len / len(musti[musti.target == i])\n",
    "    print(frac, i)\n",
    "    musti = musti.drop(musti.query(f'target == {i}').sample(frac=frac).index)\n",
    "\n",
    "print(len(musti[musti.target == 2]), len(musti[musti.target == 1]), len(musti[musti.target == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set Shape: (2112, 56320)\n"
     ]
    }
   ],
   "source": [
    "X, y = musti.drop('target', axis=1), musti['target']\n",
    "y = y.astype(np.uint8)  # less RAM space\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Training set Shape: {X_train.shape}')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model\n",
      "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
      "             param_grid=[{'max_features': [2, 4, 8],\n",
      "                          'n_estimators': [10, 100, 200],\n",
      "                          'random_state': [42]}],\n",
      "             verbose=2)\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "print('fitting model')\n",
    "#model = model.fit(X_train, y_train)\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [10, 100, 200], 'max_features': [2, 4, 8], 'random_state': [42]},\n",
    "    ]\n",
    "\n",
    "gridsearch = GridSearchCV(model, param_grid, cv=3, verbose=2)\n",
    "print(gridsearch)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print(gridsearch.best_params_,gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the chosen model and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cross_val_score(model, X_test, y_test, cv=3)\n",
    "print(f'\\t{a}')\n",
    "print(f'\\tmean: {np.mean(a)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5121bd3a35215f255f04af5486badf7493b805d644bc99080ec709e2d9bd47d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Python3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
